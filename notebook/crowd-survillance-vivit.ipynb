{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9801971,"sourceType":"datasetVersion","datasetId":6007558},{"sourceId":9805310,"sourceType":"datasetVersion","datasetId":6010078},{"sourceId":9805319,"sourceType":"datasetVersion","datasetId":6010086},{"sourceId":9805384,"sourceType":"datasetVersion","datasetId":6010137}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Flatten, Dense, BatchNormalization\nfrom tensorflow.keras.models import Sequential\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom transformers import ViTModel\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import ViTForImageClassification\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:11:14.900994Z","iopub.execute_input":"2025-05-14T17:11:14.901201Z","iopub.status.idle":"2025-05-14T17:11:32.244345Z","shell.execute_reply.started":"2025-05-14T17:11:14.901178Z","shell.execute_reply":"2025-05-14T17:11:32.243329Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade tensorflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:11:36.691848Z","iopub.execute_input":"2025-05-14T17:11:36.692179Z","iopub.status.idle":"2025-05-14T17:12:50.598964Z","shell.execute_reply.started":"2025-05-14T17:11:36.692151Z","shell.execute_reply":"2025-05-14T17:12:50.597916Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.16.1)\nCollecting tensorflow\n  Downloading tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (70.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.62.2)\nCollecting tensorboard~=2.19.0 (from tensorflow)\n  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting keras>=3.5.0 (from tensorflow)\n  Downloading keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.11.0)\nCollecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n  Downloading ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.37.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.6)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.5)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\nDownloading tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.8/644.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading keras-3.9.2-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, keras, tensorflow\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.3.2\n    Uninstalling ml-dtypes-0.3.2:\n      Successfully uninstalled ml-dtypes-0.3.2\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.16.2\n    Uninstalling tensorboard-2.16.2:\n      Successfully uninstalled tensorboard-2.16.2\n  Attempting uninstall: keras\n    Found existing installation: keras 3.3.3\n    Uninstalling keras-3.3.3:\n      Successfully uninstalled keras-3.3.3\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.16.1\n    Uninstalling tensorflow-2.16.1:\n      Successfully uninstalled tensorflow-2.16.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.9.1 requires tensorflow~=2.16.1, but you have tensorflow 2.19.0 which is incompatible.\ntensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.19.0 which is incompatible.\ntf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-3.9.2 ml-dtypes-0.5.1 tensorboard-2.19.0 tensorflow-2.19.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install torch torchvision transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:12:50.601065Z","iopub.execute_input":"2025-05-14T17:12:50.601386Z","iopub.status.idle":"2025-05-14T17:12:58.778236Z","shell.execute_reply.started":"2025-05-14T17:12:50.601355Z","shell.execute_reply":"2025-05-14T17:12:58.777056Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install torchvision transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:12:58.779926Z","iopub.execute_input":"2025-05-14T17:12:58.780845Z","iopub.status.idle":"2025-05-14T17:13:06.950204Z","shell.execute_reply.started":"2025-05-14T17:12:58.780775Z","shell.execute_reply":"2025-05-14T17:13:06.949058Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.4.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchvision) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchvision) (1.3.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"pip install torchinfo\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:13:06.952418Z","iopub.execute_input":"2025-05-14T17:13:06.952726Z","iopub.status.idle":"2025-05-14T17:13:15.329925Z","shell.execute_reply.started":"2025-05-14T17:13:06.952697Z","shell.execute_reply":"2025-05-14T17:13:15.328854Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchinfo in /opt/conda/lib/python3.10/site-packages (1.8.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"pip install torchsummary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:13:17.438949Z","iopub.execute_input":"2025-05-14T17:13:17.439613Z","iopub.status.idle":"2025-05-14T17:13:25.925936Z","shell.execute_reply.started":"2025-05-14T17:13:17.439577Z","shell.execute_reply":"2025-05-14T17:13:25.924693Z"}},"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm  # For the progress bar\nfrom transformers import ViTForImageClassification\nfrom torchsummary import summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:13:25.928332Z","iopub.execute_input":"2025-05-14T17:13:25.929113Z","iopub.status.idle":"2025-05-14T17:13:25.937890Z","shell.execute_reply.started":"2025-05-14T17:13:25.929037Z","shell.execute_reply":"2025-05-14T17:13:25.937012Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Define image transformation for PyTorch\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Define Dataset class\nclass ViolenceDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = []\n        self.labels = []\n        label_map = {'violence': 1, 'non_violence': 0}\n        for class_dir in os.listdir(root_dir):\n            class_dir_path = os.path.join(root_dir, class_dir)\n            if class_dir in label_map:\n                for sub_dir in os.listdir(class_dir_path):\n                    sub_dir_path = os.path.join(class_dir_path, sub_dir)\n                    if os.path.isdir(sub_dir_path):\n                        for img_name in os.listdir(sub_dir_path):\n                            img_path = os.path.join(sub_dir_path, img_name)\n                            if os.path.isfile(img_path) and img_name.lower().endswith(('.jpg')):\n                                self.images.append(img_path)\n                                self.labels.append(label_map[class_dir])\n        print(f'Loaded {len(self.images)} images from {root_dir}')\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.labels[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# Instantiate datasets and loaders\ntrain_dataset = ViolenceDataset(root_dir='/kaggle/input/annotated-frame-for-violence-detection/final_frames/train', transform=transform)\nval_dataset = ViolenceDataset(root_dir='/kaggle/input/annotated-frame-for-violence-detection/final_frames/validation', transform=transform)\ntest_dataset = ViolenceDataset(root_dir='/kaggle/input/annotated-frame-for-violence-detection/final_frames/test', transform=transform)\n\nbatch_size = 8\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:13:25.938958Z","iopub.execute_input":"2025-05-14T17:13:25.939274Z","iopub.status.idle":"2025-05-14T17:13:59.651303Z","shell.execute_reply.started":"2025-05-14T17:13:25.939248Z","shell.execute_reply":"2025-05-14T17:13:59.650373Z"}},"outputs":[{"name":"stdout","text":"Loaded 23491 images from /kaggle/input/annotated-frame-for-violence-detection/final_frames/train\nLoaded 4150 images from /kaggle/input/annotated-frame-for-violence-detection/final_frames/validation\nLoaded 4784 images from /kaggle/input/annotated-frame-for-violence-detection/final_frames/test\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from collections import Counter\n\ndef count_classes(dataset):\n    # Use Counter to count occurrences of each label\n    labels = [label for _, label in dataset]\n    class_counts = Counter(labels)\n    return class_counts\nprint(count_classes(train_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:14:03.762334Z","iopub.execute_input":"2025-05-14T17:14:03.763218Z","iopub.status.idle":"2025-05-14T17:17:00.366530Z","shell.execute_reply.started":"2025-05-14T17:14:03.763181Z","shell.execute_reply":"2025-05-14T17:17:00.365595Z"}},"outputs":[{"name":"stdout","text":"Counter({0: 13368, 1: 10123})\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Define your ViVIT model\nclass ViVITModel(nn.Module):\n    def __init__(self, num_classes=2, dropout_rate=0.1):\n        super(ViVITModel, self).__init__()\n        self.vit = ViTForImageClassification.from_pretrained(\n            'google/vit-base-patch16-224',\n            num_labels=num_classes,\n            ignore_mismatched_sizes=True\n        )\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        x = self.vit(x).logits\n        return self.dropout(x)\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create model instance\nmodel = ViVITModel(num_classes=2, dropout_rate=0.1)\nmodel.to(device)\n\n# Print model summary\nbatch_size = 1\nsummary(model, input_size=(batch_size, 3, 224, 224), device=device.type)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:05:02.834099Z","iopub.execute_input":"2025-05-14T20:05:02.834433Z","iopub.status.idle":"2025-05-14T20:05:05.068611Z","shell.execute_reply.started":"2025-05-14T20:05:02.834407Z","shell.execute_reply":"2025-05-14T20:05:05.067457Z"}},"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"===================================================================================================================\nLayer (type:depth-idx)                                            Output Shape              Param #\n===================================================================================================================\nViVITModel                                                        [1, 2]                    --\n├─ViTForImageClassification: 1-1                                  [1, 2]                    --\n│    └─ViTModel: 2-1                                              [1, 197, 768]             --\n│    │    └─ViTEmbeddings: 3-1                                    [1, 197, 768]             742,656\n│    │    └─ViTEncoder: 3-2                                       [1, 197, 768]             85,054,464\n│    │    └─LayerNorm: 3-3                                        [1, 197, 768]             1,536\n│    └─Linear: 2-2                                                [1, 2]                    1,538\n├─Dropout: 1-2                                                    [1, 2]                    --\n===================================================================================================================\nTotal params: 85,800,194\nTrainable params: 85,800,194\nNon-trainable params: 0\nTotal mult-adds (M): 200.81\n===================================================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 162.18\nParams size (MB): 342.59\nEstimated Total Size (MB): 505.38\n==================================================================================================================="},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom transformers import ViTForImageClassification\nfrom torchinfo import summary\n\n# Define your ViVIT model\nclass ViVITModel(nn.Module):\n    def __init__(self, num_classes=2, dropout_rate=0.1):\n        super(ViVITModel, self).__init__()\n        self.vit = ViTForImageClassification.from_pretrained(\n            'google/vit-base-patch16-224',\n            num_labels=num_classes,\n            ignore_mismatched_sizes=True\n        )\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        x = self.vit(x).logits\n        return self.dropout(x)\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create model instance\nmodel = ViVITModel(num_classes=2, dropout_rate=0.1)\nmodel.to(device)\n\n# Print model summary\nbatch_size = 1\nsummary(model, input_size=(batch_size, 3, 224, 224), device=device.type)\nprint(model)\n\n# Define data transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load datasets (make sure to change the path to your dataset)\ntrain_dataset = datasets.ImageFolder(root='/kaggle/input/annotated-frame-for-violence-detection/final_frames/train', transform=transform)\nval_dataset = datasets.ImageFolder(root='/kaggle/input/annotated-frame-for-violence-detection/final_frames/validation', transform=transform)\ntest_dataset = datasets.ImageFolder(root='/kaggle/input/annotated-frame-for-violence-detection/final_frames/test', transform=transform)\n\n# Create data loaders\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n\n# Define training function\ndef train_model(model, train_loader, val_loader, num_epochs):\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.01)\n    criterion = nn.CrossEntropyLoss()\n    model.to(device)\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss, correct_train_preds, total_train_preds = 0, 0, 0\n        \n        # Training Phase\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            train_loss += loss.item()\n            \n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            \n            # Track training accuracy\n            _, predicted = torch.max(outputs, 1)\n            correct_train_preds += (predicted == labels).sum().item()\n            total_train_preds += labels.size(0)\n        \n        # Calculate metrics for the epoch\n        avg_train_loss = train_loss / len(train_loader)\n        train_accuracy = correct_train_preds / total_train_preds\n        \n        # Validation Phase\n        val_accuracy = evaluate_model(model, val_loader)\n        \n        # Print epoch details\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, \"\n              f\"Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\n# Define evaluation function\ndef evaluate_model(model, loader):\n    model.eval()\n    correct_preds, total_preds = 0, 0\n\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            correct_preds += (predicted == labels).sum().item()\n            total_preds += labels.size(0)\n\n    accuracy = correct_preds / total_preds\n    return accuracy\n\n# Start training\nnum_epochs = 10  # Set the number of epochs\ntrain_model(model, train_loader, val_loader, num_epochs)\n\n# Evaluate on test data\ntest_accuracy = evaluate_model(model, test_loader)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:17:13.916483Z","iopub.execute_input":"2025-05-14T17:17:13.917271Z","iopub.status.idle":"2025-05-14T20:04:35.454199Z","shell.execute_reply.started":"2025-05-14T17:17:13.917234Z","shell.execute_reply":"2025-05-14T20:04:35.453165Z"}},"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"ViVITModel(\n  (vit): ViTForImageClassification(\n    (vit): ViTModel(\n      (embeddings): ViTEmbeddings(\n        (patch_embeddings): ViTPatchEmbeddings(\n          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (encoder): ViTEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x ViTLayer(\n            (attention): ViTSdpaAttention(\n              (attention): ViTSdpaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): ViTSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (intermediate): ViTIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): ViTOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          )\n        )\n      )\n      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    )\n    (classifier): Linear(in_features=768, out_features=2, bias=True)\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nEpoch [1/10], Train Loss: 0.0446, Train Accuracy: 0.9854, Validation Accuracy: 0.9195\nEpoch [2/10], Train Loss: 0.0928, Train Accuracy: 0.9692, Validation Accuracy: 0.8366\nEpoch [3/10], Train Loss: 0.0730, Train Accuracy: 0.9753, Validation Accuracy: 0.8995\nEpoch [4/10], Train Loss: 0.0594, Train Accuracy: 0.9798, Validation Accuracy: 0.9000\nEpoch [5/10], Train Loss: 0.0416, Train Accuracy: 0.9865, Validation Accuracy: 0.8892\nEpoch [6/10], Train Loss: 0.0275, Train Accuracy: 0.9903, Validation Accuracy: 0.9716\nEpoch [7/10], Train Loss: 0.0401, Train Accuracy: 0.9854, Validation Accuracy: 0.9723\nEpoch [8/10], Train Loss: 0.0328, Train Accuracy: 0.9882, Validation Accuracy: 0.9581\nEpoch [9/10], Train Loss: 0.0197, Train Accuracy: 0.9925, Validation Accuracy: 0.9660\nEpoch [10/10], Train Loss: 0.0712, Train Accuracy: 0.9722, Validation Accuracy: 0.8771\nTest Accuracy: 0.9672\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n# Define function to get predictions and labels\ndef get_predictions_and_labels(model, loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    return np.array(all_preds), np.array(all_labels)\n\n# Get predictions and labels for test set\ntest_preds, test_labels = get_predictions_and_labels(model, test_loader)\n\n# Generate classification report\nprint(\"Classification Report:\")\nprint(classification_report(test_labels, test_preds, target_names=test_dataset.classes))\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(test_labels, test_preds)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Plot confusion matrix\nplt.figure(figsize=(6, 6))\nplt.imshow(conf_matrix, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.colorbar()\ntick_marks = np.arange(len(test_dataset.classes))\nplt.xticks(tick_marks, test_dataset.classes, rotation=45)\nplt.yticks(tick_marks, test_dataset.classes)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\n\n# ROC-AUC Curve (for binary classification)\nif len(test_dataset.classes) == 2:  # Ensure binary classification\n    test_probs = []\n    with torch.no_grad():\n        for images, _ in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            probabilities = torch.softmax(outputs, dim=1)[:, 1]  # Get probabilities for class 1\n            test_probs.extend(probabilities.cpu().numpy())\n\n    roc_auc = roc_auc_score(test_labels, test_probs)\n    fpr, tpr, _ = roc_curve(test_labels, test_probs)\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.4f})')\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc='lower right')\n    plt.show()\n\n    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\nelse:\n    print(\"ROC-AUC Curve is only applicable for binary classification.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:05:15.502385Z","iopub.execute_input":"2025-05-14T20:05:15.503209Z","iopub.status.idle":"2025-05-14T20:08:00.994216Z","shell.execute_reply.started":"2025-05-14T20:05:15.503173Z","shell.execute_reply":"2025-05-14T20:08:00.993377Z"}},"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\nnon_violence       0.51      0.55      0.53      2439\n    violence       0.49      0.45      0.47      2345\n\n    accuracy                           0.50      4784\n   macro avg       0.50      0.50      0.50      4784\nweighted avg       0.50      0.50      0.50      4784\n\nConfusion Matrix:\n[[1348 1091]\n [1280 1065]]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x600 with 2 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"ROC-AUC Score: 0.5073\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Define the path to save the model\nmodel_save_path = \"SafeVision_vivit.h5\"\n\n# Save the model's state_dict\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:08:00.995929Z","iopub.execute_input":"2025-05-14T20:08:00.996291Z","iopub.status.idle":"2025-05-14T20:08:01.424441Z","shell.execute_reply.started":"2025-05-14T20:08:00.996253Z","shell.execute_reply":"2025-05-14T20:08:01.423446Z"}},"outputs":[{"name":"stdout","text":"Model saved to SafeVision_vivit.h5\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"video_path = '/kaggle/input/v-videos/V_996.mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:08:01.425552Z","iopub.execute_input":"2025-05-14T20:08:01.425866Z","iopub.status.idle":"2025-05-14T20:08:01.429750Z","shell.execute_reply.started":"2025-05-14T20:08:01.425839Z","shell.execute_reply":"2025-05-14T20:08:01.429016Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:08:01.432269Z","iopub.execute_input":"2025-05-14T20:08:01.432949Z","iopub.status.idle":"2025-05-14T20:08:01.439776Z","shell.execute_reply.started":"2025-05-14T20:08:01.432923Z","shell.execute_reply":"2025-05-14T20:08:01.438991Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nfrom torchvision import transforms\nfrom sklearn.metrics import precision_recall_curve\n\n# Define transformations for frames\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Frame-level prediction and temporal smoothing function\ndef predict_violence(video_path, model, frame_skip=1, smoothing_window=5, device='cpu'):\n    cap = cv2.VideoCapture(video_path)\n    frame_predictions = []\n\n    # Ensure model is in evaluation mode\n    model.eval()\n    \n    with torch.no_grad():\n        while True:\n            for _ in range(frame_skip):\n                ret, frame = cap.read()\n                if not ret:\n                    break\n            \n            if not ret:\n                break\n\n            # Preprocess and predict violence probability for each frame\n            frame_transformed = transform(frame).unsqueeze(0).to(device)\n            outputs = model(frame_transformed)\n            probabilities = torch.softmax(outputs, dim=1)\n            violence_prob = probabilities[0][1].item()\n            frame_predictions.append(violence_prob)\n    \n    cap.release()\n    \n    # Apply temporal smoothing to predictions\n    smoothed_predictions = np.convolve(frame_predictions, np.ones(smoothing_window) / smoothing_window, mode='valid')\n    average_prediction = np.mean(smoothed_predictions)\n    print(f\"Average Smoothed Violence Probability: {average_prediction:.4f}\")\n\n    # Determine video classification based on threshold\n    threshold = fine_tune_threshold(model, val_loader, device)\n    print(f\"Optimal Threshold: {threshold:.4f}\")\n\n    if average_prediction >= 0.5:\n        print(\"Prediction: Violence\")\n    else:\n        print(\"Prediction: Non-Violence\")\n\n# Function to find optimal threshold using validation set\ndef fine_tune_threshold(model, val_loader, device='cpu'):\n    model.eval()\n    all_probs = []\n    all_labels = []\n\n    # Get probabilities and true labels for validation set\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n            all_probs.extend(probs)\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calculate precision-recall and determine best threshold\n    precision, recall, thresholds = precision_recall_curve(all_labels, all_probs)\n    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-7)  # Avoid division by zero\n    best_threshold = thresholds[np.argmax(f1_scores)]\n    return best_threshold\n\n# Example Usage\n# Assuming `model` is loaded and `val_loader` is defined for validation data\nvideo_path =  '/kaggle/input/violence/V_862.mp4'\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Call function to predict violence in a video\npredict_violence(video_path, model, frame_skip=1, smoothing_window=5, device=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:08:01.440717Z","iopub.execute_input":"2025-05-14T20:08:01.441048Z","iopub.status.idle":"2025-05-14T20:09:18.613265Z","shell.execute_reply.started":"2025-05-14T20:08:01.441023Z","shell.execute_reply":"2025-05-14T20:09:18.612287Z"}},"outputs":[{"name":"stdout","text":"Average Smoothed Violence Probability: 0.7037\nOptimal Threshold: 0.5143\nPrediction: Violence\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import cv2\nimport torch\nfrom torchvision import transforms\nfrom transformers import ViTForImageClassification\nfrom PIL import Image\n\nimport torch\nfrom transformers import ViTForImageClassification\n\n# Load your trained model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = ViTForImageClassification.from_pretrained(\n    'google/vit-base-patch16-224',\n    num_labels=2,  # Set the number of labels for your task\n    ignore_mismatched_sizes=True\n)\n\n# Load the state dict\nstate_dict = torch.load('SafeVision_vivit.h5', map_location=device)\n\n# Modify the state_dict to remove the classifier\nstate_dict.pop('classifier.weight', None)\nstate_dict.pop('classifier.bias', None)\n\n# Load the modified state dict into the model\nmodel.load_state_dict(state_dict, strict=False)  # Use strict=False to ignore classifier keys\nmodel.to(device)\nmodel.eval()\n\n\n\n# Define the same transformation used during training\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef predict_frame(frame):\n    image = Image.fromarray(frame).convert('RGB')\n    image = transform(image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        outputs = model(image)\n    \n    # Access the logits directly\n    logits = outputs.logits  # Get the logits\n    return torch.argmax(logits, dim=1).item()  # Get the predicted class index\n\n\ndef analyze_video(video_path):\n    cap = cv2.VideoCapture(video_path)\n    total_frames = 0\n    violence_frames = 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        prediction = predict_frame(frame)\n        total_frames += 1\n        if prediction == 1:  # Assuming '1' corresponds to violence\n            violence_frames += 1\n    \n    cap.release()\n    \n    if total_frames > 0:\n        violence_percentage = (violence_frames / total_frames) * 100\n        print(f\"Total Frames: {total_frames}, Violent Frames: {violence_frames}, Violence Percentage: {violence_percentage:.2f}%\")\n    else:\n        print(\"No frames to analyze.\")\n\n# Example usage\nvideo_path = '/kaggle/input/nv-videos/NV_999.mp4'  # Ensure this path is correct\nanalyze_video(video_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:09:18.614612Z","iopub.execute_input":"2025-05-14T20:09:18.615089Z","iopub.status.idle":"2025-05-14T20:09:24.125408Z","shell.execute_reply.started":"2025-05-14T20:09:18.615048Z","shell.execute_reply":"2025-05-14T20:09:24.124380Z"}},"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_30/1142728325.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load('SafeVision_vivit.h5', map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Total Frames: 168, Violent Frames: 75, Violence Percentage: 44.64%\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:09:24.126494Z","iopub.execute_input":"2025-05-14T20:09:24.126773Z","iopub.status.idle":"2025-05-14T20:09:24.132275Z","shell.execute_reply.started":"2025-05-14T20:09:24.126747Z","shell.execute_reply":"2025-05-14T20:09:24.131421Z"}},"outputs":[{"name":"stdout","text":"ViTForImageClassification(\n  (vit): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTSdpaAttention(\n            (attention): ViTSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"pretrained_dict = torch.load('SafeVision_vivit.h5', map_location=device)\nmodel_dict = model.state_dict()\n\n# Print keys\nprint(\"Keys in the pretrained model:\")\nprint(pretrained_dict.keys())\n\nprint(\"Keys in the current model:\")\nprint(model_dict.keys())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:09:24.133475Z","iopub.execute_input":"2025-05-14T20:09:24.133940Z","iopub.status.idle":"2025-05-14T20:09:24.338077Z","shell.execute_reply.started":"2025-05-14T20:09:24.133901Z","shell.execute_reply":"2025-05-14T20:09:24.337250Z"}},"outputs":[{"name":"stdout","text":"Keys in the pretrained model:\nodict_keys(['vit.vit.embeddings.cls_token', 'vit.vit.embeddings.position_embeddings', 'vit.vit.embeddings.patch_embeddings.projection.weight', 'vit.vit.embeddings.patch_embeddings.projection.bias', 'vit.vit.encoder.layer.0.attention.attention.query.weight', 'vit.vit.encoder.layer.0.attention.attention.query.bias', 'vit.vit.encoder.layer.0.attention.attention.key.weight', 'vit.vit.encoder.layer.0.attention.attention.key.bias', 'vit.vit.encoder.layer.0.attention.attention.value.weight', 'vit.vit.encoder.layer.0.attention.attention.value.bias', 'vit.vit.encoder.layer.0.attention.output.dense.weight', 'vit.vit.encoder.layer.0.attention.output.dense.bias', 'vit.vit.encoder.layer.0.intermediate.dense.weight', 'vit.vit.encoder.layer.0.intermediate.dense.bias', 'vit.vit.encoder.layer.0.output.dense.weight', 'vit.vit.encoder.layer.0.output.dense.bias', 'vit.vit.encoder.layer.0.layernorm_before.weight', 'vit.vit.encoder.layer.0.layernorm_before.bias', 'vit.vit.encoder.layer.0.layernorm_after.weight', 'vit.vit.encoder.layer.0.layernorm_after.bias', 'vit.vit.encoder.layer.1.attention.attention.query.weight', 'vit.vit.encoder.layer.1.attention.attention.query.bias', 'vit.vit.encoder.layer.1.attention.attention.key.weight', 'vit.vit.encoder.layer.1.attention.attention.key.bias', 'vit.vit.encoder.layer.1.attention.attention.value.weight', 'vit.vit.encoder.layer.1.attention.attention.value.bias', 'vit.vit.encoder.layer.1.attention.output.dense.weight', 'vit.vit.encoder.layer.1.attention.output.dense.bias', 'vit.vit.encoder.layer.1.intermediate.dense.weight', 'vit.vit.encoder.layer.1.intermediate.dense.bias', 'vit.vit.encoder.layer.1.output.dense.weight', 'vit.vit.encoder.layer.1.output.dense.bias', 'vit.vit.encoder.layer.1.layernorm_before.weight', 'vit.vit.encoder.layer.1.layernorm_before.bias', 'vit.vit.encoder.layer.1.layernorm_after.weight', 'vit.vit.encoder.layer.1.layernorm_after.bias', 'vit.vit.encoder.layer.2.attention.attention.query.weight', 'vit.vit.encoder.layer.2.attention.attention.query.bias', 'vit.vit.encoder.layer.2.attention.attention.key.weight', 'vit.vit.encoder.layer.2.attention.attention.key.bias', 'vit.vit.encoder.layer.2.attention.attention.value.weight', 'vit.vit.encoder.layer.2.attention.attention.value.bias', 'vit.vit.encoder.layer.2.attention.output.dense.weight', 'vit.vit.encoder.layer.2.attention.output.dense.bias', 'vit.vit.encoder.layer.2.intermediate.dense.weight', 'vit.vit.encoder.layer.2.intermediate.dense.bias', 'vit.vit.encoder.layer.2.output.dense.weight', 'vit.vit.encoder.layer.2.output.dense.bias', 'vit.vit.encoder.layer.2.layernorm_before.weight', 'vit.vit.encoder.layer.2.layernorm_before.bias', 'vit.vit.encoder.layer.2.layernorm_after.weight', 'vit.vit.encoder.layer.2.layernorm_after.bias', 'vit.vit.encoder.layer.3.attention.attention.query.weight', 'vit.vit.encoder.layer.3.attention.attention.query.bias', 'vit.vit.encoder.layer.3.attention.attention.key.weight', 'vit.vit.encoder.layer.3.attention.attention.key.bias', 'vit.vit.encoder.layer.3.attention.attention.value.weight', 'vit.vit.encoder.layer.3.attention.attention.value.bias', 'vit.vit.encoder.layer.3.attention.output.dense.weight', 'vit.vit.encoder.layer.3.attention.output.dense.bias', 'vit.vit.encoder.layer.3.intermediate.dense.weight', 'vit.vit.encoder.layer.3.intermediate.dense.bias', 'vit.vit.encoder.layer.3.output.dense.weight', 'vit.vit.encoder.layer.3.output.dense.bias', 'vit.vit.encoder.layer.3.layernorm_before.weight', 'vit.vit.encoder.layer.3.layernorm_before.bias', 'vit.vit.encoder.layer.3.layernorm_after.weight', 'vit.vit.encoder.layer.3.layernorm_after.bias', 'vit.vit.encoder.layer.4.attention.attention.query.weight', 'vit.vit.encoder.layer.4.attention.attention.query.bias', 'vit.vit.encoder.layer.4.attention.attention.key.weight', 'vit.vit.encoder.layer.4.attention.attention.key.bias', 'vit.vit.encoder.layer.4.attention.attention.value.weight', 'vit.vit.encoder.layer.4.attention.attention.value.bias', 'vit.vit.encoder.layer.4.attention.output.dense.weight', 'vit.vit.encoder.layer.4.attention.output.dense.bias', 'vit.vit.encoder.layer.4.intermediate.dense.weight', 'vit.vit.encoder.layer.4.intermediate.dense.bias', 'vit.vit.encoder.layer.4.output.dense.weight', 'vit.vit.encoder.layer.4.output.dense.bias', 'vit.vit.encoder.layer.4.layernorm_before.weight', 'vit.vit.encoder.layer.4.layernorm_before.bias', 'vit.vit.encoder.layer.4.layernorm_after.weight', 'vit.vit.encoder.layer.4.layernorm_after.bias', 'vit.vit.encoder.layer.5.attention.attention.query.weight', 'vit.vit.encoder.layer.5.attention.attention.query.bias', 'vit.vit.encoder.layer.5.attention.attention.key.weight', 'vit.vit.encoder.layer.5.attention.attention.key.bias', 'vit.vit.encoder.layer.5.attention.attention.value.weight', 'vit.vit.encoder.layer.5.attention.attention.value.bias', 'vit.vit.encoder.layer.5.attention.output.dense.weight', 'vit.vit.encoder.layer.5.attention.output.dense.bias', 'vit.vit.encoder.layer.5.intermediate.dense.weight', 'vit.vit.encoder.layer.5.intermediate.dense.bias', 'vit.vit.encoder.layer.5.output.dense.weight', 'vit.vit.encoder.layer.5.output.dense.bias', 'vit.vit.encoder.layer.5.layernorm_before.weight', 'vit.vit.encoder.layer.5.layernorm_before.bias', 'vit.vit.encoder.layer.5.layernorm_after.weight', 'vit.vit.encoder.layer.5.layernorm_after.bias', 'vit.vit.encoder.layer.6.attention.attention.query.weight', 'vit.vit.encoder.layer.6.attention.attention.query.bias', 'vit.vit.encoder.layer.6.attention.attention.key.weight', 'vit.vit.encoder.layer.6.attention.attention.key.bias', 'vit.vit.encoder.layer.6.attention.attention.value.weight', 'vit.vit.encoder.layer.6.attention.attention.value.bias', 'vit.vit.encoder.layer.6.attention.output.dense.weight', 'vit.vit.encoder.layer.6.attention.output.dense.bias', 'vit.vit.encoder.layer.6.intermediate.dense.weight', 'vit.vit.encoder.layer.6.intermediate.dense.bias', 'vit.vit.encoder.layer.6.output.dense.weight', 'vit.vit.encoder.layer.6.output.dense.bias', 'vit.vit.encoder.layer.6.layernorm_before.weight', 'vit.vit.encoder.layer.6.layernorm_before.bias', 'vit.vit.encoder.layer.6.layernorm_after.weight', 'vit.vit.encoder.layer.6.layernorm_after.bias', 'vit.vit.encoder.layer.7.attention.attention.query.weight', 'vit.vit.encoder.layer.7.attention.attention.query.bias', 'vit.vit.encoder.layer.7.attention.attention.key.weight', 'vit.vit.encoder.layer.7.attention.attention.key.bias', 'vit.vit.encoder.layer.7.attention.attention.value.weight', 'vit.vit.encoder.layer.7.attention.attention.value.bias', 'vit.vit.encoder.layer.7.attention.output.dense.weight', 'vit.vit.encoder.layer.7.attention.output.dense.bias', 'vit.vit.encoder.layer.7.intermediate.dense.weight', 'vit.vit.encoder.layer.7.intermediate.dense.bias', 'vit.vit.encoder.layer.7.output.dense.weight', 'vit.vit.encoder.layer.7.output.dense.bias', 'vit.vit.encoder.layer.7.layernorm_before.weight', 'vit.vit.encoder.layer.7.layernorm_before.bias', 'vit.vit.encoder.layer.7.layernorm_after.weight', 'vit.vit.encoder.layer.7.layernorm_after.bias', 'vit.vit.encoder.layer.8.attention.attention.query.weight', 'vit.vit.encoder.layer.8.attention.attention.query.bias', 'vit.vit.encoder.layer.8.attention.attention.key.weight', 'vit.vit.encoder.layer.8.attention.attention.key.bias', 'vit.vit.encoder.layer.8.attention.attention.value.weight', 'vit.vit.encoder.layer.8.attention.attention.value.bias', 'vit.vit.encoder.layer.8.attention.output.dense.weight', 'vit.vit.encoder.layer.8.attention.output.dense.bias', 'vit.vit.encoder.layer.8.intermediate.dense.weight', 'vit.vit.encoder.layer.8.intermediate.dense.bias', 'vit.vit.encoder.layer.8.output.dense.weight', 'vit.vit.encoder.layer.8.output.dense.bias', 'vit.vit.encoder.layer.8.layernorm_before.weight', 'vit.vit.encoder.layer.8.layernorm_before.bias', 'vit.vit.encoder.layer.8.layernorm_after.weight', 'vit.vit.encoder.layer.8.layernorm_after.bias', 'vit.vit.encoder.layer.9.attention.attention.query.weight', 'vit.vit.encoder.layer.9.attention.attention.query.bias', 'vit.vit.encoder.layer.9.attention.attention.key.weight', 'vit.vit.encoder.layer.9.attention.attention.key.bias', 'vit.vit.encoder.layer.9.attention.attention.value.weight', 'vit.vit.encoder.layer.9.attention.attention.value.bias', 'vit.vit.encoder.layer.9.attention.output.dense.weight', 'vit.vit.encoder.layer.9.attention.output.dense.bias', 'vit.vit.encoder.layer.9.intermediate.dense.weight', 'vit.vit.encoder.layer.9.intermediate.dense.bias', 'vit.vit.encoder.layer.9.output.dense.weight', 'vit.vit.encoder.layer.9.output.dense.bias', 'vit.vit.encoder.layer.9.layernorm_before.weight', 'vit.vit.encoder.layer.9.layernorm_before.bias', 'vit.vit.encoder.layer.9.layernorm_after.weight', 'vit.vit.encoder.layer.9.layernorm_after.bias', 'vit.vit.encoder.layer.10.attention.attention.query.weight', 'vit.vit.encoder.layer.10.attention.attention.query.bias', 'vit.vit.encoder.layer.10.attention.attention.key.weight', 'vit.vit.encoder.layer.10.attention.attention.key.bias', 'vit.vit.encoder.layer.10.attention.attention.value.weight', 'vit.vit.encoder.layer.10.attention.attention.value.bias', 'vit.vit.encoder.layer.10.attention.output.dense.weight', 'vit.vit.encoder.layer.10.attention.output.dense.bias', 'vit.vit.encoder.layer.10.intermediate.dense.weight', 'vit.vit.encoder.layer.10.intermediate.dense.bias', 'vit.vit.encoder.layer.10.output.dense.weight', 'vit.vit.encoder.layer.10.output.dense.bias', 'vit.vit.encoder.layer.10.layernorm_before.weight', 'vit.vit.encoder.layer.10.layernorm_before.bias', 'vit.vit.encoder.layer.10.layernorm_after.weight', 'vit.vit.encoder.layer.10.layernorm_after.bias', 'vit.vit.encoder.layer.11.attention.attention.query.weight', 'vit.vit.encoder.layer.11.attention.attention.query.bias', 'vit.vit.encoder.layer.11.attention.attention.key.weight', 'vit.vit.encoder.layer.11.attention.attention.key.bias', 'vit.vit.encoder.layer.11.attention.attention.value.weight', 'vit.vit.encoder.layer.11.attention.attention.value.bias', 'vit.vit.encoder.layer.11.attention.output.dense.weight', 'vit.vit.encoder.layer.11.attention.output.dense.bias', 'vit.vit.encoder.layer.11.intermediate.dense.weight', 'vit.vit.encoder.layer.11.intermediate.dense.bias', 'vit.vit.encoder.layer.11.output.dense.weight', 'vit.vit.encoder.layer.11.output.dense.bias', 'vit.vit.encoder.layer.11.layernorm_before.weight', 'vit.vit.encoder.layer.11.layernorm_before.bias', 'vit.vit.encoder.layer.11.layernorm_after.weight', 'vit.vit.encoder.layer.11.layernorm_after.bias', 'vit.vit.layernorm.weight', 'vit.vit.layernorm.bias', 'vit.classifier.weight', 'vit.classifier.bias'])\nKeys in the current model:\nodict_keys(['vit.embeddings.cls_token', 'vit.embeddings.position_embeddings', 'vit.embeddings.patch_embeddings.projection.weight', 'vit.embeddings.patch_embeddings.projection.bias', 'vit.encoder.layer.0.attention.attention.query.weight', 'vit.encoder.layer.0.attention.attention.query.bias', 'vit.encoder.layer.0.attention.attention.key.weight', 'vit.encoder.layer.0.attention.attention.key.bias', 'vit.encoder.layer.0.attention.attention.value.weight', 'vit.encoder.layer.0.attention.attention.value.bias', 'vit.encoder.layer.0.attention.output.dense.weight', 'vit.encoder.layer.0.attention.output.dense.bias', 'vit.encoder.layer.0.intermediate.dense.weight', 'vit.encoder.layer.0.intermediate.dense.bias', 'vit.encoder.layer.0.output.dense.weight', 'vit.encoder.layer.0.output.dense.bias', 'vit.encoder.layer.0.layernorm_before.weight', 'vit.encoder.layer.0.layernorm_before.bias', 'vit.encoder.layer.0.layernorm_after.weight', 'vit.encoder.layer.0.layernorm_after.bias', 'vit.encoder.layer.1.attention.attention.query.weight', 'vit.encoder.layer.1.attention.attention.query.bias', 'vit.encoder.layer.1.attention.attention.key.weight', 'vit.encoder.layer.1.attention.attention.key.bias', 'vit.encoder.layer.1.attention.attention.value.weight', 'vit.encoder.layer.1.attention.attention.value.bias', 'vit.encoder.layer.1.attention.output.dense.weight', 'vit.encoder.layer.1.attention.output.dense.bias', 'vit.encoder.layer.1.intermediate.dense.weight', 'vit.encoder.layer.1.intermediate.dense.bias', 'vit.encoder.layer.1.output.dense.weight', 'vit.encoder.layer.1.output.dense.bias', 'vit.encoder.layer.1.layernorm_before.weight', 'vit.encoder.layer.1.layernorm_before.bias', 'vit.encoder.layer.1.layernorm_after.weight', 'vit.encoder.layer.1.layernorm_after.bias', 'vit.encoder.layer.2.attention.attention.query.weight', 'vit.encoder.layer.2.attention.attention.query.bias', 'vit.encoder.layer.2.attention.attention.key.weight', 'vit.encoder.layer.2.attention.attention.key.bias', 'vit.encoder.layer.2.attention.attention.value.weight', 'vit.encoder.layer.2.attention.attention.value.bias', 'vit.encoder.layer.2.attention.output.dense.weight', 'vit.encoder.layer.2.attention.output.dense.bias', 'vit.encoder.layer.2.intermediate.dense.weight', 'vit.encoder.layer.2.intermediate.dense.bias', 'vit.encoder.layer.2.output.dense.weight', 'vit.encoder.layer.2.output.dense.bias', 'vit.encoder.layer.2.layernorm_before.weight', 'vit.encoder.layer.2.layernorm_before.bias', 'vit.encoder.layer.2.layernorm_after.weight', 'vit.encoder.layer.2.layernorm_after.bias', 'vit.encoder.layer.3.attention.attention.query.weight', 'vit.encoder.layer.3.attention.attention.query.bias', 'vit.encoder.layer.3.attention.attention.key.weight', 'vit.encoder.layer.3.attention.attention.key.bias', 'vit.encoder.layer.3.attention.attention.value.weight', 'vit.encoder.layer.3.attention.attention.value.bias', 'vit.encoder.layer.3.attention.output.dense.weight', 'vit.encoder.layer.3.attention.output.dense.bias', 'vit.encoder.layer.3.intermediate.dense.weight', 'vit.encoder.layer.3.intermediate.dense.bias', 'vit.encoder.layer.3.output.dense.weight', 'vit.encoder.layer.3.output.dense.bias', 'vit.encoder.layer.3.layernorm_before.weight', 'vit.encoder.layer.3.layernorm_before.bias', 'vit.encoder.layer.3.layernorm_after.weight', 'vit.encoder.layer.3.layernorm_after.bias', 'vit.encoder.layer.4.attention.attention.query.weight', 'vit.encoder.layer.4.attention.attention.query.bias', 'vit.encoder.layer.4.attention.attention.key.weight', 'vit.encoder.layer.4.attention.attention.key.bias', 'vit.encoder.layer.4.attention.attention.value.weight', 'vit.encoder.layer.4.attention.attention.value.bias', 'vit.encoder.layer.4.attention.output.dense.weight', 'vit.encoder.layer.4.attention.output.dense.bias', 'vit.encoder.layer.4.intermediate.dense.weight', 'vit.encoder.layer.4.intermediate.dense.bias', 'vit.encoder.layer.4.output.dense.weight', 'vit.encoder.layer.4.output.dense.bias', 'vit.encoder.layer.4.layernorm_before.weight', 'vit.encoder.layer.4.layernorm_before.bias', 'vit.encoder.layer.4.layernorm_after.weight', 'vit.encoder.layer.4.layernorm_after.bias', 'vit.encoder.layer.5.attention.attention.query.weight', 'vit.encoder.layer.5.attention.attention.query.bias', 'vit.encoder.layer.5.attention.attention.key.weight', 'vit.encoder.layer.5.attention.attention.key.bias', 'vit.encoder.layer.5.attention.attention.value.weight', 'vit.encoder.layer.5.attention.attention.value.bias', 'vit.encoder.layer.5.attention.output.dense.weight', 'vit.encoder.layer.5.attention.output.dense.bias', 'vit.encoder.layer.5.intermediate.dense.weight', 'vit.encoder.layer.5.intermediate.dense.bias', 'vit.encoder.layer.5.output.dense.weight', 'vit.encoder.layer.5.output.dense.bias', 'vit.encoder.layer.5.layernorm_before.weight', 'vit.encoder.layer.5.layernorm_before.bias', 'vit.encoder.layer.5.layernorm_after.weight', 'vit.encoder.layer.5.layernorm_after.bias', 'vit.encoder.layer.6.attention.attention.query.weight', 'vit.encoder.layer.6.attention.attention.query.bias', 'vit.encoder.layer.6.attention.attention.key.weight', 'vit.encoder.layer.6.attention.attention.key.bias', 'vit.encoder.layer.6.attention.attention.value.weight', 'vit.encoder.layer.6.attention.attention.value.bias', 'vit.encoder.layer.6.attention.output.dense.weight', 'vit.encoder.layer.6.attention.output.dense.bias', 'vit.encoder.layer.6.intermediate.dense.weight', 'vit.encoder.layer.6.intermediate.dense.bias', 'vit.encoder.layer.6.output.dense.weight', 'vit.encoder.layer.6.output.dense.bias', 'vit.encoder.layer.6.layernorm_before.weight', 'vit.encoder.layer.6.layernorm_before.bias', 'vit.encoder.layer.6.layernorm_after.weight', 'vit.encoder.layer.6.layernorm_after.bias', 'vit.encoder.layer.7.attention.attention.query.weight', 'vit.encoder.layer.7.attention.attention.query.bias', 'vit.encoder.layer.7.attention.attention.key.weight', 'vit.encoder.layer.7.attention.attention.key.bias', 'vit.encoder.layer.7.attention.attention.value.weight', 'vit.encoder.layer.7.attention.attention.value.bias', 'vit.encoder.layer.7.attention.output.dense.weight', 'vit.encoder.layer.7.attention.output.dense.bias', 'vit.encoder.layer.7.intermediate.dense.weight', 'vit.encoder.layer.7.intermediate.dense.bias', 'vit.encoder.layer.7.output.dense.weight', 'vit.encoder.layer.7.output.dense.bias', 'vit.encoder.layer.7.layernorm_before.weight', 'vit.encoder.layer.7.layernorm_before.bias', 'vit.encoder.layer.7.layernorm_after.weight', 'vit.encoder.layer.7.layernorm_after.bias', 'vit.encoder.layer.8.attention.attention.query.weight', 'vit.encoder.layer.8.attention.attention.query.bias', 'vit.encoder.layer.8.attention.attention.key.weight', 'vit.encoder.layer.8.attention.attention.key.bias', 'vit.encoder.layer.8.attention.attention.value.weight', 'vit.encoder.layer.8.attention.attention.value.bias', 'vit.encoder.layer.8.attention.output.dense.weight', 'vit.encoder.layer.8.attention.output.dense.bias', 'vit.encoder.layer.8.intermediate.dense.weight', 'vit.encoder.layer.8.intermediate.dense.bias', 'vit.encoder.layer.8.output.dense.weight', 'vit.encoder.layer.8.output.dense.bias', 'vit.encoder.layer.8.layernorm_before.weight', 'vit.encoder.layer.8.layernorm_before.bias', 'vit.encoder.layer.8.layernorm_after.weight', 'vit.encoder.layer.8.layernorm_after.bias', 'vit.encoder.layer.9.attention.attention.query.weight', 'vit.encoder.layer.9.attention.attention.query.bias', 'vit.encoder.layer.9.attention.attention.key.weight', 'vit.encoder.layer.9.attention.attention.key.bias', 'vit.encoder.layer.9.attention.attention.value.weight', 'vit.encoder.layer.9.attention.attention.value.bias', 'vit.encoder.layer.9.attention.output.dense.weight', 'vit.encoder.layer.9.attention.output.dense.bias', 'vit.encoder.layer.9.intermediate.dense.weight', 'vit.encoder.layer.9.intermediate.dense.bias', 'vit.encoder.layer.9.output.dense.weight', 'vit.encoder.layer.9.output.dense.bias', 'vit.encoder.layer.9.layernorm_before.weight', 'vit.encoder.layer.9.layernorm_before.bias', 'vit.encoder.layer.9.layernorm_after.weight', 'vit.encoder.layer.9.layernorm_after.bias', 'vit.encoder.layer.10.attention.attention.query.weight', 'vit.encoder.layer.10.attention.attention.query.bias', 'vit.encoder.layer.10.attention.attention.key.weight', 'vit.encoder.layer.10.attention.attention.key.bias', 'vit.encoder.layer.10.attention.attention.value.weight', 'vit.encoder.layer.10.attention.attention.value.bias', 'vit.encoder.layer.10.attention.output.dense.weight', 'vit.encoder.layer.10.attention.output.dense.bias', 'vit.encoder.layer.10.intermediate.dense.weight', 'vit.encoder.layer.10.intermediate.dense.bias', 'vit.encoder.layer.10.output.dense.weight', 'vit.encoder.layer.10.output.dense.bias', 'vit.encoder.layer.10.layernorm_before.weight', 'vit.encoder.layer.10.layernorm_before.bias', 'vit.encoder.layer.10.layernorm_after.weight', 'vit.encoder.layer.10.layernorm_after.bias', 'vit.encoder.layer.11.attention.attention.query.weight', 'vit.encoder.layer.11.attention.attention.query.bias', 'vit.encoder.layer.11.attention.attention.key.weight', 'vit.encoder.layer.11.attention.attention.key.bias', 'vit.encoder.layer.11.attention.attention.value.weight', 'vit.encoder.layer.11.attention.attention.value.bias', 'vit.encoder.layer.11.attention.output.dense.weight', 'vit.encoder.layer.11.attention.output.dense.bias', 'vit.encoder.layer.11.intermediate.dense.weight', 'vit.encoder.layer.11.intermediate.dense.bias', 'vit.encoder.layer.11.output.dense.weight', 'vit.encoder.layer.11.output.dense.bias', 'vit.encoder.layer.11.layernorm_before.weight', 'vit.encoder.layer.11.layernorm_before.bias', 'vit.encoder.layer.11.layernorm_after.weight', 'vit.encoder.layer.11.layernorm_after.bias', 'vit.layernorm.weight', 'vit.layernorm.bias', 'classifier.weight', 'classifier.bias'])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/1202934361.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  pretrained_dict = torch.load('SafeVision_vivit.h5', map_location=device)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"for key in model_dict.keys():\n    if key not in pretrained_dict:\n        print(f\"Missing key: {key}\")\n    elif pretrained_dict[key].shape != model_dict[key].shape:\n        print(f\"Shape mismatch for key: {key} | Pretrained shape: {pretrained_dict[key].shape}, Current shape: {model_dict[key].shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:09:24.339158Z","iopub.execute_input":"2025-05-14T20:09:24.339421Z","iopub.status.idle":"2025-05-14T20:09:24.345820Z","shell.execute_reply.started":"2025-05-14T20:09:24.339397Z","shell.execute_reply":"2025-05-14T20:09:24.344856Z"}},"outputs":[{"name":"stdout","text":"Missing key: vit.embeddings.cls_token\nMissing key: vit.embeddings.position_embeddings\nMissing key: vit.embeddings.patch_embeddings.projection.weight\nMissing key: vit.embeddings.patch_embeddings.projection.bias\nMissing key: vit.encoder.layer.0.attention.attention.query.weight\nMissing key: vit.encoder.layer.0.attention.attention.query.bias\nMissing key: vit.encoder.layer.0.attention.attention.key.weight\nMissing key: vit.encoder.layer.0.attention.attention.key.bias\nMissing key: vit.encoder.layer.0.attention.attention.value.weight\nMissing key: vit.encoder.layer.0.attention.attention.value.bias\nMissing key: vit.encoder.layer.0.attention.output.dense.weight\nMissing key: vit.encoder.layer.0.attention.output.dense.bias\nMissing key: vit.encoder.layer.0.intermediate.dense.weight\nMissing key: vit.encoder.layer.0.intermediate.dense.bias\nMissing key: vit.encoder.layer.0.output.dense.weight\nMissing key: vit.encoder.layer.0.output.dense.bias\nMissing key: vit.encoder.layer.0.layernorm_before.weight\nMissing key: vit.encoder.layer.0.layernorm_before.bias\nMissing key: vit.encoder.layer.0.layernorm_after.weight\nMissing key: vit.encoder.layer.0.layernorm_after.bias\nMissing key: vit.encoder.layer.1.attention.attention.query.weight\nMissing key: vit.encoder.layer.1.attention.attention.query.bias\nMissing key: vit.encoder.layer.1.attention.attention.key.weight\nMissing key: vit.encoder.layer.1.attention.attention.key.bias\nMissing key: vit.encoder.layer.1.attention.attention.value.weight\nMissing key: vit.encoder.layer.1.attention.attention.value.bias\nMissing key: vit.encoder.layer.1.attention.output.dense.weight\nMissing key: vit.encoder.layer.1.attention.output.dense.bias\nMissing key: vit.encoder.layer.1.intermediate.dense.weight\nMissing key: vit.encoder.layer.1.intermediate.dense.bias\nMissing key: vit.encoder.layer.1.output.dense.weight\nMissing key: vit.encoder.layer.1.output.dense.bias\nMissing key: vit.encoder.layer.1.layernorm_before.weight\nMissing key: vit.encoder.layer.1.layernorm_before.bias\nMissing key: vit.encoder.layer.1.layernorm_after.weight\nMissing key: vit.encoder.layer.1.layernorm_after.bias\nMissing key: vit.encoder.layer.2.attention.attention.query.weight\nMissing key: vit.encoder.layer.2.attention.attention.query.bias\nMissing key: vit.encoder.layer.2.attention.attention.key.weight\nMissing key: vit.encoder.layer.2.attention.attention.key.bias\nMissing key: vit.encoder.layer.2.attention.attention.value.weight\nMissing key: vit.encoder.layer.2.attention.attention.value.bias\nMissing key: vit.encoder.layer.2.attention.output.dense.weight\nMissing key: vit.encoder.layer.2.attention.output.dense.bias\nMissing key: vit.encoder.layer.2.intermediate.dense.weight\nMissing key: vit.encoder.layer.2.intermediate.dense.bias\nMissing key: vit.encoder.layer.2.output.dense.weight\nMissing key: vit.encoder.layer.2.output.dense.bias\nMissing key: vit.encoder.layer.2.layernorm_before.weight\nMissing key: vit.encoder.layer.2.layernorm_before.bias\nMissing key: vit.encoder.layer.2.layernorm_after.weight\nMissing key: vit.encoder.layer.2.layernorm_after.bias\nMissing key: vit.encoder.layer.3.attention.attention.query.weight\nMissing key: vit.encoder.layer.3.attention.attention.query.bias\nMissing key: vit.encoder.layer.3.attention.attention.key.weight\nMissing key: vit.encoder.layer.3.attention.attention.key.bias\nMissing key: vit.encoder.layer.3.attention.attention.value.weight\nMissing key: vit.encoder.layer.3.attention.attention.value.bias\nMissing key: vit.encoder.layer.3.attention.output.dense.weight\nMissing key: vit.encoder.layer.3.attention.output.dense.bias\nMissing key: vit.encoder.layer.3.intermediate.dense.weight\nMissing key: vit.encoder.layer.3.intermediate.dense.bias\nMissing key: vit.encoder.layer.3.output.dense.weight\nMissing key: vit.encoder.layer.3.output.dense.bias\nMissing key: vit.encoder.layer.3.layernorm_before.weight\nMissing key: vit.encoder.layer.3.layernorm_before.bias\nMissing key: vit.encoder.layer.3.layernorm_after.weight\nMissing key: vit.encoder.layer.3.layernorm_after.bias\nMissing key: vit.encoder.layer.4.attention.attention.query.weight\nMissing key: vit.encoder.layer.4.attention.attention.query.bias\nMissing key: vit.encoder.layer.4.attention.attention.key.weight\nMissing key: vit.encoder.layer.4.attention.attention.key.bias\nMissing key: vit.encoder.layer.4.attention.attention.value.weight\nMissing key: vit.encoder.layer.4.attention.attention.value.bias\nMissing key: vit.encoder.layer.4.attention.output.dense.weight\nMissing key: vit.encoder.layer.4.attention.output.dense.bias\nMissing key: vit.encoder.layer.4.intermediate.dense.weight\nMissing key: vit.encoder.layer.4.intermediate.dense.bias\nMissing key: vit.encoder.layer.4.output.dense.weight\nMissing key: vit.encoder.layer.4.output.dense.bias\nMissing key: vit.encoder.layer.4.layernorm_before.weight\nMissing key: vit.encoder.layer.4.layernorm_before.bias\nMissing key: vit.encoder.layer.4.layernorm_after.weight\nMissing key: vit.encoder.layer.4.layernorm_after.bias\nMissing key: vit.encoder.layer.5.attention.attention.query.weight\nMissing key: vit.encoder.layer.5.attention.attention.query.bias\nMissing key: vit.encoder.layer.5.attention.attention.key.weight\nMissing key: vit.encoder.layer.5.attention.attention.key.bias\nMissing key: vit.encoder.layer.5.attention.attention.value.weight\nMissing key: vit.encoder.layer.5.attention.attention.value.bias\nMissing key: vit.encoder.layer.5.attention.output.dense.weight\nMissing key: vit.encoder.layer.5.attention.output.dense.bias\nMissing key: vit.encoder.layer.5.intermediate.dense.weight\nMissing key: vit.encoder.layer.5.intermediate.dense.bias\nMissing key: vit.encoder.layer.5.output.dense.weight\nMissing key: vit.encoder.layer.5.output.dense.bias\nMissing key: vit.encoder.layer.5.layernorm_before.weight\nMissing key: vit.encoder.layer.5.layernorm_before.bias\nMissing key: vit.encoder.layer.5.layernorm_after.weight\nMissing key: vit.encoder.layer.5.layernorm_after.bias\nMissing key: vit.encoder.layer.6.attention.attention.query.weight\nMissing key: vit.encoder.layer.6.attention.attention.query.bias\nMissing key: vit.encoder.layer.6.attention.attention.key.weight\nMissing key: vit.encoder.layer.6.attention.attention.key.bias\nMissing key: vit.encoder.layer.6.attention.attention.value.weight\nMissing key: vit.encoder.layer.6.attention.attention.value.bias\nMissing key: vit.encoder.layer.6.attention.output.dense.weight\nMissing key: vit.encoder.layer.6.attention.output.dense.bias\nMissing key: vit.encoder.layer.6.intermediate.dense.weight\nMissing key: vit.encoder.layer.6.intermediate.dense.bias\nMissing key: vit.encoder.layer.6.output.dense.weight\nMissing key: vit.encoder.layer.6.output.dense.bias\nMissing key: vit.encoder.layer.6.layernorm_before.weight\nMissing key: vit.encoder.layer.6.layernorm_before.bias\nMissing key: vit.encoder.layer.6.layernorm_after.weight\nMissing key: vit.encoder.layer.6.layernorm_after.bias\nMissing key: vit.encoder.layer.7.attention.attention.query.weight\nMissing key: vit.encoder.layer.7.attention.attention.query.bias\nMissing key: vit.encoder.layer.7.attention.attention.key.weight\nMissing key: vit.encoder.layer.7.attention.attention.key.bias\nMissing key: vit.encoder.layer.7.attention.attention.value.weight\nMissing key: vit.encoder.layer.7.attention.attention.value.bias\nMissing key: vit.encoder.layer.7.attention.output.dense.weight\nMissing key: vit.encoder.layer.7.attention.output.dense.bias\nMissing key: vit.encoder.layer.7.intermediate.dense.weight\nMissing key: vit.encoder.layer.7.intermediate.dense.bias\nMissing key: vit.encoder.layer.7.output.dense.weight\nMissing key: vit.encoder.layer.7.output.dense.bias\nMissing key: vit.encoder.layer.7.layernorm_before.weight\nMissing key: vit.encoder.layer.7.layernorm_before.bias\nMissing key: vit.encoder.layer.7.layernorm_after.weight\nMissing key: vit.encoder.layer.7.layernorm_after.bias\nMissing key: vit.encoder.layer.8.attention.attention.query.weight\nMissing key: vit.encoder.layer.8.attention.attention.query.bias\nMissing key: vit.encoder.layer.8.attention.attention.key.weight\nMissing key: vit.encoder.layer.8.attention.attention.key.bias\nMissing key: vit.encoder.layer.8.attention.attention.value.weight\nMissing key: vit.encoder.layer.8.attention.attention.value.bias\nMissing key: vit.encoder.layer.8.attention.output.dense.weight\nMissing key: vit.encoder.layer.8.attention.output.dense.bias\nMissing key: vit.encoder.layer.8.intermediate.dense.weight\nMissing key: vit.encoder.layer.8.intermediate.dense.bias\nMissing key: vit.encoder.layer.8.output.dense.weight\nMissing key: vit.encoder.layer.8.output.dense.bias\nMissing key: vit.encoder.layer.8.layernorm_before.weight\nMissing key: vit.encoder.layer.8.layernorm_before.bias\nMissing key: vit.encoder.layer.8.layernorm_after.weight\nMissing key: vit.encoder.layer.8.layernorm_after.bias\nMissing key: vit.encoder.layer.9.attention.attention.query.weight\nMissing key: vit.encoder.layer.9.attention.attention.query.bias\nMissing key: vit.encoder.layer.9.attention.attention.key.weight\nMissing key: vit.encoder.layer.9.attention.attention.key.bias\nMissing key: vit.encoder.layer.9.attention.attention.value.weight\nMissing key: vit.encoder.layer.9.attention.attention.value.bias\nMissing key: vit.encoder.layer.9.attention.output.dense.weight\nMissing key: vit.encoder.layer.9.attention.output.dense.bias\nMissing key: vit.encoder.layer.9.intermediate.dense.weight\nMissing key: vit.encoder.layer.9.intermediate.dense.bias\nMissing key: vit.encoder.layer.9.output.dense.weight\nMissing key: vit.encoder.layer.9.output.dense.bias\nMissing key: vit.encoder.layer.9.layernorm_before.weight\nMissing key: vit.encoder.layer.9.layernorm_before.bias\nMissing key: vit.encoder.layer.9.layernorm_after.weight\nMissing key: vit.encoder.layer.9.layernorm_after.bias\nMissing key: vit.encoder.layer.10.attention.attention.query.weight\nMissing key: vit.encoder.layer.10.attention.attention.query.bias\nMissing key: vit.encoder.layer.10.attention.attention.key.weight\nMissing key: vit.encoder.layer.10.attention.attention.key.bias\nMissing key: vit.encoder.layer.10.attention.attention.value.weight\nMissing key: vit.encoder.layer.10.attention.attention.value.bias\nMissing key: vit.encoder.layer.10.attention.output.dense.weight\nMissing key: vit.encoder.layer.10.attention.output.dense.bias\nMissing key: vit.encoder.layer.10.intermediate.dense.weight\nMissing key: vit.encoder.layer.10.intermediate.dense.bias\nMissing key: vit.encoder.layer.10.output.dense.weight\nMissing key: vit.encoder.layer.10.output.dense.bias\nMissing key: vit.encoder.layer.10.layernorm_before.weight\nMissing key: vit.encoder.layer.10.layernorm_before.bias\nMissing key: vit.encoder.layer.10.layernorm_after.weight\nMissing key: vit.encoder.layer.10.layernorm_after.bias\nMissing key: vit.encoder.layer.11.attention.attention.query.weight\nMissing key: vit.encoder.layer.11.attention.attention.query.bias\nMissing key: vit.encoder.layer.11.attention.attention.key.weight\nMissing key: vit.encoder.layer.11.attention.attention.key.bias\nMissing key: vit.encoder.layer.11.attention.attention.value.weight\nMissing key: vit.encoder.layer.11.attention.attention.value.bias\nMissing key: vit.encoder.layer.11.attention.output.dense.weight\nMissing key: vit.encoder.layer.11.attention.output.dense.bias\nMissing key: vit.encoder.layer.11.intermediate.dense.weight\nMissing key: vit.encoder.layer.11.intermediate.dense.bias\nMissing key: vit.encoder.layer.11.output.dense.weight\nMissing key: vit.encoder.layer.11.output.dense.bias\nMissing key: vit.encoder.layer.11.layernorm_before.weight\nMissing key: vit.encoder.layer.11.layernorm_before.bias\nMissing key: vit.encoder.layer.11.layernorm_after.weight\nMissing key: vit.encoder.layer.11.layernorm_after.bias\nMissing key: vit.layernorm.weight\nMissing key: vit.layernorm.bias\nMissing key: classifier.weight\nMissing key: classifier.bias\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"for key in pretrained_dict.keys():\n    if key not in model_dict:\n        print(f\"Unexpected key: {key}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:09:24.347736Z","iopub.execute_input":"2025-05-14T20:09:24.348069Z","iopub.status.idle":"2025-05-14T20:09:24.355416Z","shell.execute_reply.started":"2025-05-14T20:09:24.348023Z","shell.execute_reply":"2025-05-14T20:09:24.354704Z"}},"outputs":[{"name":"stdout","text":"Unexpected key: vit.vit.embeddings.cls_token\nUnexpected key: vit.vit.embeddings.position_embeddings\nUnexpected key: vit.vit.embeddings.patch_embeddings.projection.weight\nUnexpected key: vit.vit.embeddings.patch_embeddings.projection.bias\nUnexpected key: vit.vit.encoder.layer.0.attention.attention.query.weight\nUnexpected key: vit.vit.encoder.layer.0.attention.attention.query.bias\nUnexpected key: vit.vit.encoder.layer.0.attention.attention.key.weight\nUnexpected key: vit.vit.encoder.layer.0.attention.attention.key.bias\nUnexpected key: vit.vit.encoder.layer.0.attention.attention.value.weight\nUnexpected key: vit.vit.encoder.layer.0.attention.attention.value.bias\nUnexpected key: vit.vit.encoder.layer.0.attention.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.0.attention.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.0.intermediate.dense.weight\nUnexpected key: vit.vit.encoder.layer.0.intermediate.dense.bias\nUnexpected key: vit.vit.encoder.layer.0.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.0.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.0.layernorm_before.weight\nUnexpected key: vit.vit.encoder.layer.0.layernorm_before.bias\nUnexpected key: vit.vit.encoder.layer.0.layernorm_after.weight\nUnexpected key: vit.vit.encoder.layer.0.layernorm_after.bias\nUnexpected key: vit.vit.encoder.layer.1.attention.attention.query.weight\nUnexpected key: vit.vit.encoder.layer.1.attention.attention.query.bias\nUnexpected key: vit.vit.encoder.layer.1.attention.attention.key.weight\nUnexpected key: vit.vit.encoder.layer.1.attention.attention.key.bias\nUnexpected key: vit.vit.encoder.layer.1.attention.attention.value.weight\nUnexpected key: vit.vit.encoder.layer.1.attention.attention.value.bias\nUnexpected key: vit.vit.encoder.layer.1.attention.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.1.attention.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.1.intermediate.dense.weight\nUnexpected key: vit.vit.encoder.layer.1.intermediate.dense.bias\nUnexpected key: vit.vit.encoder.layer.1.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.1.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.1.layernorm_before.weight\nUnexpected key: vit.vit.encoder.layer.1.layernorm_before.bias\nUnexpected key: vit.vit.encoder.layer.1.layernorm_after.weight\nUnexpected key: vit.vit.encoder.layer.1.layernorm_after.bias\nUnexpected key: vit.vit.encoder.layer.2.attention.attention.query.weight\nUnexpected key: vit.vit.encoder.layer.2.attention.attention.query.bias\nUnexpected key: vit.vit.encoder.layer.2.attention.attention.key.weight\nUnexpected key: vit.vit.encoder.layer.2.attention.attention.key.bias\nUnexpected key: vit.vit.encoder.layer.2.attention.attention.value.weight\nUnexpected key: vit.vit.encoder.layer.2.attention.attention.value.bias\nUnexpected key: vit.vit.encoder.layer.2.attention.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.2.attention.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.2.intermediate.dense.weight\nUnexpected key: vit.vit.encoder.layer.2.intermediate.dense.bias\nUnexpected key: vit.vit.encoder.layer.2.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.2.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.2.layernorm_before.weight\nUnexpected key: vit.vit.encoder.layer.2.layernorm_before.bias\nUnexpected key: vit.vit.encoder.layer.2.layernorm_after.weight\nUnexpected key: vit.vit.encoder.layer.2.layernorm_after.bias\nUnexpected key: vit.vit.encoder.layer.3.attention.attention.query.weight\nUnexpected key: vit.vit.encoder.layer.3.attention.attention.query.bias\nUnexpected key: vit.vit.encoder.layer.3.attention.attention.key.weight\nUnexpected key: vit.vit.encoder.layer.3.attention.attention.key.bias\nUnexpected key: vit.vit.encoder.layer.3.attention.attention.value.weight\nUnexpected key: vit.vit.encoder.layer.3.attention.attention.value.bias\nUnexpected key: vit.vit.encoder.layer.3.attention.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.3.attention.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.3.intermediate.dense.weight\nUnexpected key: vit.vit.encoder.layer.3.intermediate.dense.bias\nUnexpected key: vit.vit.encoder.layer.3.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.3.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.3.layernorm_before.weight\nUnexpected key: vit.vit.encoder.layer.3.layernorm_before.bias\nUnexpected key: vit.vit.encoder.layer.3.layernorm_after.weight\nUnexpected key: vit.vit.encoder.layer.3.layernorm_after.bias\nUnexpected key: vit.vit.encoder.layer.4.attention.attention.query.weight\nUnexpected key: vit.vit.encoder.layer.4.attention.attention.query.bias\nUnexpected key: vit.vit.encoder.layer.4.attention.attention.key.weight\nUnexpected key: vit.vit.encoder.layer.4.attention.attention.key.bias\nUnexpected key: vit.vit.encoder.layer.4.attention.attention.value.weight\nUnexpected key: vit.vit.encoder.layer.4.attention.attention.value.bias\nUnexpected key: vit.vit.encoder.layer.4.attention.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.4.attention.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.4.intermediate.dense.weight\nUnexpected key: vit.vit.encoder.layer.4.intermediate.dense.bias\nUnexpected key: vit.vit.encoder.layer.4.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.4.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.4.layernorm_before.weight\nUnexpected key: vit.vit.encoder.layer.4.layernorm_before.bias\nUnexpected key: vit.vit.encoder.layer.4.layernorm_after.weight\nUnexpected key: vit.vit.encoder.layer.4.layernorm_after.bias\nUnexpected key: vit.vit.encoder.layer.5.attention.attention.query.weight\nUnexpected key: vit.vit.encoder.layer.5.attention.attention.query.bias\nUnexpected key: vit.vit.encoder.layer.5.attention.attention.key.weight\nUnexpected key: vit.vit.encoder.layer.5.attention.attention.key.bias\nUnexpected key: vit.vit.encoder.layer.5.attention.attention.value.weight\nUnexpected key: vit.vit.encoder.layer.5.attention.attention.value.bias\nUnexpected key: vit.vit.encoder.layer.5.attention.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.5.attention.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.5.intermediate.dense.weight\nUnexpected key: vit.vit.encoder.layer.5.intermediate.dense.bias\nUnexpected key: vit.vit.encoder.layer.5.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.5.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.5.layernorm_before.weight\nUnexpected key: vit.vit.encoder.layer.5.layernorm_before.bias\nUnexpected key: vit.vit.encoder.layer.5.layernorm_after.weight\nUnexpected key: vit.vit.encoder.layer.5.layernorm_after.bias\nUnexpected key: vit.vit.encoder.layer.6.attention.attention.query.weight\nUnexpected key: vit.vit.encoder.layer.6.attention.attention.query.bias\nUnexpected key: vit.vit.encoder.layer.6.attention.attention.key.weight\nUnexpected key: vit.vit.encoder.layer.6.attention.attention.key.bias\nUnexpected key: vit.vit.encoder.layer.6.attention.attention.value.weight\nUnexpected key: vit.vit.encoder.layer.6.attention.attention.value.bias\nUnexpected key: vit.vit.encoder.layer.6.attention.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.6.attention.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.6.intermediate.dense.weight\nUnexpected key: vit.vit.encoder.layer.6.intermediate.dense.bias\nUnexpected key: vit.vit.encoder.layer.6.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.6.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.6.layernorm_before.weight\nUnexpected key: vit.vit.encoder.layer.6.layernorm_before.bias\nUnexpected key: vit.vit.encoder.layer.6.layernorm_after.weight\nUnexpected key: vit.vit.encoder.layer.6.layernorm_after.bias\nUnexpected key: vit.vit.encoder.layer.7.attention.attention.query.weight\nUnexpected key: vit.vit.encoder.layer.7.attention.attention.query.bias\nUnexpected key: vit.vit.encoder.layer.7.attention.attention.key.weight\nUnexpected key: vit.vit.encoder.layer.7.attention.attention.key.bias\nUnexpected key: vit.vit.encoder.layer.7.attention.attention.value.weight\nUnexpected key: vit.vit.encoder.layer.7.attention.attention.value.bias\nUnexpected key: vit.vit.encoder.layer.7.attention.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.7.attention.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.7.intermediate.dense.weight\nUnexpected key: vit.vit.encoder.layer.7.intermediate.dense.bias\nUnexpected key: vit.vit.encoder.layer.7.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.7.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.7.layernorm_before.weight\nUnexpected key: vit.vit.encoder.layer.7.layernorm_before.bias\nUnexpected key: vit.vit.encoder.layer.7.layernorm_after.weight\nUnexpected key: vit.vit.encoder.layer.7.layernorm_after.bias\nUnexpected key: vit.vit.encoder.layer.8.attention.attention.query.weight\nUnexpected key: vit.vit.encoder.layer.8.attention.attention.query.bias\nUnexpected key: vit.vit.encoder.layer.8.attention.attention.key.weight\nUnexpected key: vit.vit.encoder.layer.8.attention.attention.key.bias\nUnexpected key: vit.vit.encoder.layer.8.attention.attention.value.weight\nUnexpected key: vit.vit.encoder.layer.8.attention.attention.value.bias\nUnexpected key: vit.vit.encoder.layer.8.attention.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.8.attention.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.8.intermediate.dense.weight\nUnexpected key: vit.vit.encoder.layer.8.intermediate.dense.bias\nUnexpected key: vit.vit.encoder.layer.8.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.8.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.8.layernorm_before.weight\nUnexpected key: vit.vit.encoder.layer.8.layernorm_before.bias\nUnexpected key: vit.vit.encoder.layer.8.layernorm_after.weight\nUnexpected key: vit.vit.encoder.layer.8.layernorm_after.bias\nUnexpected key: vit.vit.encoder.layer.9.attention.attention.query.weight\nUnexpected key: vit.vit.encoder.layer.9.attention.attention.query.bias\nUnexpected key: vit.vit.encoder.layer.9.attention.attention.key.weight\nUnexpected key: vit.vit.encoder.layer.9.attention.attention.key.bias\nUnexpected key: vit.vit.encoder.layer.9.attention.attention.value.weight\nUnexpected key: vit.vit.encoder.layer.9.attention.attention.value.bias\nUnexpected key: vit.vit.encoder.layer.9.attention.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.9.attention.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.9.intermediate.dense.weight\nUnexpected key: vit.vit.encoder.layer.9.intermediate.dense.bias\nUnexpected key: vit.vit.encoder.layer.9.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.9.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.9.layernorm_before.weight\nUnexpected key: vit.vit.encoder.layer.9.layernorm_before.bias\nUnexpected key: vit.vit.encoder.layer.9.layernorm_after.weight\nUnexpected key: vit.vit.encoder.layer.9.layernorm_after.bias\nUnexpected key: vit.vit.encoder.layer.10.attention.attention.query.weight\nUnexpected key: vit.vit.encoder.layer.10.attention.attention.query.bias\nUnexpected key: vit.vit.encoder.layer.10.attention.attention.key.weight\nUnexpected key: vit.vit.encoder.layer.10.attention.attention.key.bias\nUnexpected key: vit.vit.encoder.layer.10.attention.attention.value.weight\nUnexpected key: vit.vit.encoder.layer.10.attention.attention.value.bias\nUnexpected key: vit.vit.encoder.layer.10.attention.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.10.attention.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.10.intermediate.dense.weight\nUnexpected key: vit.vit.encoder.layer.10.intermediate.dense.bias\nUnexpected key: vit.vit.encoder.layer.10.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.10.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.10.layernorm_before.weight\nUnexpected key: vit.vit.encoder.layer.10.layernorm_before.bias\nUnexpected key: vit.vit.encoder.layer.10.layernorm_after.weight\nUnexpected key: vit.vit.encoder.layer.10.layernorm_after.bias\nUnexpected key: vit.vit.encoder.layer.11.attention.attention.query.weight\nUnexpected key: vit.vit.encoder.layer.11.attention.attention.query.bias\nUnexpected key: vit.vit.encoder.layer.11.attention.attention.key.weight\nUnexpected key: vit.vit.encoder.layer.11.attention.attention.key.bias\nUnexpected key: vit.vit.encoder.layer.11.attention.attention.value.weight\nUnexpected key: vit.vit.encoder.layer.11.attention.attention.value.bias\nUnexpected key: vit.vit.encoder.layer.11.attention.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.11.attention.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.11.intermediate.dense.weight\nUnexpected key: vit.vit.encoder.layer.11.intermediate.dense.bias\nUnexpected key: vit.vit.encoder.layer.11.output.dense.weight\nUnexpected key: vit.vit.encoder.layer.11.output.dense.bias\nUnexpected key: vit.vit.encoder.layer.11.layernorm_before.weight\nUnexpected key: vit.vit.encoder.layer.11.layernorm_before.bias\nUnexpected key: vit.vit.encoder.layer.11.layernorm_after.weight\nUnexpected key: vit.vit.encoder.layer.11.layernorm_after.bias\nUnexpected key: vit.vit.layernorm.weight\nUnexpected key: vit.vit.layernorm.bias\nUnexpected key: vit.classifier.weight\nUnexpected key: vit.classifier.bias\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"for key, value in model_dict.items():\n    print(f\"{key}: {value.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:09:24.356284Z","iopub.execute_input":"2025-05-14T20:09:24.356603Z","iopub.status.idle":"2025-05-14T20:09:24.366153Z","shell.execute_reply.started":"2025-05-14T20:09:24.356568Z","shell.execute_reply":"2025-05-14T20:09:24.365330Z"}},"outputs":[{"name":"stdout","text":"vit.embeddings.cls_token: torch.Size([1, 1, 768])\nvit.embeddings.position_embeddings: torch.Size([1, 197, 768])\nvit.embeddings.patch_embeddings.projection.weight: torch.Size([768, 3, 16, 16])\nvit.embeddings.patch_embeddings.projection.bias: torch.Size([768])\nvit.encoder.layer.0.attention.attention.query.weight: torch.Size([768, 768])\nvit.encoder.layer.0.attention.attention.query.bias: torch.Size([768])\nvit.encoder.layer.0.attention.attention.key.weight: torch.Size([768, 768])\nvit.encoder.layer.0.attention.attention.key.bias: torch.Size([768])\nvit.encoder.layer.0.attention.attention.value.weight: torch.Size([768, 768])\nvit.encoder.layer.0.attention.attention.value.bias: torch.Size([768])\nvit.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])\nvit.encoder.layer.0.attention.output.dense.bias: torch.Size([768])\nvit.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])\nvit.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])\nvit.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])\nvit.encoder.layer.0.output.dense.bias: torch.Size([768])\nvit.encoder.layer.0.layernorm_before.weight: torch.Size([768])\nvit.encoder.layer.0.layernorm_before.bias: torch.Size([768])\nvit.encoder.layer.0.layernorm_after.weight: torch.Size([768])\nvit.encoder.layer.0.layernorm_after.bias: torch.Size([768])\nvit.encoder.layer.1.attention.attention.query.weight: torch.Size([768, 768])\nvit.encoder.layer.1.attention.attention.query.bias: torch.Size([768])\nvit.encoder.layer.1.attention.attention.key.weight: torch.Size([768, 768])\nvit.encoder.layer.1.attention.attention.key.bias: torch.Size([768])\nvit.encoder.layer.1.attention.attention.value.weight: torch.Size([768, 768])\nvit.encoder.layer.1.attention.attention.value.bias: torch.Size([768])\nvit.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])\nvit.encoder.layer.1.attention.output.dense.bias: torch.Size([768])\nvit.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])\nvit.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])\nvit.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])\nvit.encoder.layer.1.output.dense.bias: torch.Size([768])\nvit.encoder.layer.1.layernorm_before.weight: torch.Size([768])\nvit.encoder.layer.1.layernorm_before.bias: torch.Size([768])\nvit.encoder.layer.1.layernorm_after.weight: torch.Size([768])\nvit.encoder.layer.1.layernorm_after.bias: torch.Size([768])\nvit.encoder.layer.2.attention.attention.query.weight: torch.Size([768, 768])\nvit.encoder.layer.2.attention.attention.query.bias: torch.Size([768])\nvit.encoder.layer.2.attention.attention.key.weight: torch.Size([768, 768])\nvit.encoder.layer.2.attention.attention.key.bias: torch.Size([768])\nvit.encoder.layer.2.attention.attention.value.weight: torch.Size([768, 768])\nvit.encoder.layer.2.attention.attention.value.bias: torch.Size([768])\nvit.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])\nvit.encoder.layer.2.attention.output.dense.bias: torch.Size([768])\nvit.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])\nvit.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])\nvit.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])\nvit.encoder.layer.2.output.dense.bias: torch.Size([768])\nvit.encoder.layer.2.layernorm_before.weight: torch.Size([768])\nvit.encoder.layer.2.layernorm_before.bias: torch.Size([768])\nvit.encoder.layer.2.layernorm_after.weight: torch.Size([768])\nvit.encoder.layer.2.layernorm_after.bias: torch.Size([768])\nvit.encoder.layer.3.attention.attention.query.weight: torch.Size([768, 768])\nvit.encoder.layer.3.attention.attention.query.bias: torch.Size([768])\nvit.encoder.layer.3.attention.attention.key.weight: torch.Size([768, 768])\nvit.encoder.layer.3.attention.attention.key.bias: torch.Size([768])\nvit.encoder.layer.3.attention.attention.value.weight: torch.Size([768, 768])\nvit.encoder.layer.3.attention.attention.value.bias: torch.Size([768])\nvit.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])\nvit.encoder.layer.3.attention.output.dense.bias: torch.Size([768])\nvit.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])\nvit.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])\nvit.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])\nvit.encoder.layer.3.output.dense.bias: torch.Size([768])\nvit.encoder.layer.3.layernorm_before.weight: torch.Size([768])\nvit.encoder.layer.3.layernorm_before.bias: torch.Size([768])\nvit.encoder.layer.3.layernorm_after.weight: torch.Size([768])\nvit.encoder.layer.3.layernorm_after.bias: torch.Size([768])\nvit.encoder.layer.4.attention.attention.query.weight: torch.Size([768, 768])\nvit.encoder.layer.4.attention.attention.query.bias: torch.Size([768])\nvit.encoder.layer.4.attention.attention.key.weight: torch.Size([768, 768])\nvit.encoder.layer.4.attention.attention.key.bias: torch.Size([768])\nvit.encoder.layer.4.attention.attention.value.weight: torch.Size([768, 768])\nvit.encoder.layer.4.attention.attention.value.bias: torch.Size([768])\nvit.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])\nvit.encoder.layer.4.attention.output.dense.bias: torch.Size([768])\nvit.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])\nvit.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])\nvit.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])\nvit.encoder.layer.4.output.dense.bias: torch.Size([768])\nvit.encoder.layer.4.layernorm_before.weight: torch.Size([768])\nvit.encoder.layer.4.layernorm_before.bias: torch.Size([768])\nvit.encoder.layer.4.layernorm_after.weight: torch.Size([768])\nvit.encoder.layer.4.layernorm_after.bias: torch.Size([768])\nvit.encoder.layer.5.attention.attention.query.weight: torch.Size([768, 768])\nvit.encoder.layer.5.attention.attention.query.bias: torch.Size([768])\nvit.encoder.layer.5.attention.attention.key.weight: torch.Size([768, 768])\nvit.encoder.layer.5.attention.attention.key.bias: torch.Size([768])\nvit.encoder.layer.5.attention.attention.value.weight: torch.Size([768, 768])\nvit.encoder.layer.5.attention.attention.value.bias: torch.Size([768])\nvit.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])\nvit.encoder.layer.5.attention.output.dense.bias: torch.Size([768])\nvit.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])\nvit.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])\nvit.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])\nvit.encoder.layer.5.output.dense.bias: torch.Size([768])\nvit.encoder.layer.5.layernorm_before.weight: torch.Size([768])\nvit.encoder.layer.5.layernorm_before.bias: torch.Size([768])\nvit.encoder.layer.5.layernorm_after.weight: torch.Size([768])\nvit.encoder.layer.5.layernorm_after.bias: torch.Size([768])\nvit.encoder.layer.6.attention.attention.query.weight: torch.Size([768, 768])\nvit.encoder.layer.6.attention.attention.query.bias: torch.Size([768])\nvit.encoder.layer.6.attention.attention.key.weight: torch.Size([768, 768])\nvit.encoder.layer.6.attention.attention.key.bias: torch.Size([768])\nvit.encoder.layer.6.attention.attention.value.weight: torch.Size([768, 768])\nvit.encoder.layer.6.attention.attention.value.bias: torch.Size([768])\nvit.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])\nvit.encoder.layer.6.attention.output.dense.bias: torch.Size([768])\nvit.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])\nvit.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])\nvit.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])\nvit.encoder.layer.6.output.dense.bias: torch.Size([768])\nvit.encoder.layer.6.layernorm_before.weight: torch.Size([768])\nvit.encoder.layer.6.layernorm_before.bias: torch.Size([768])\nvit.encoder.layer.6.layernorm_after.weight: torch.Size([768])\nvit.encoder.layer.6.layernorm_after.bias: torch.Size([768])\nvit.encoder.layer.7.attention.attention.query.weight: torch.Size([768, 768])\nvit.encoder.layer.7.attention.attention.query.bias: torch.Size([768])\nvit.encoder.layer.7.attention.attention.key.weight: torch.Size([768, 768])\nvit.encoder.layer.7.attention.attention.key.bias: torch.Size([768])\nvit.encoder.layer.7.attention.attention.value.weight: torch.Size([768, 768])\nvit.encoder.layer.7.attention.attention.value.bias: torch.Size([768])\nvit.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])\nvit.encoder.layer.7.attention.output.dense.bias: torch.Size([768])\nvit.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])\nvit.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])\nvit.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])\nvit.encoder.layer.7.output.dense.bias: torch.Size([768])\nvit.encoder.layer.7.layernorm_before.weight: torch.Size([768])\nvit.encoder.layer.7.layernorm_before.bias: torch.Size([768])\nvit.encoder.layer.7.layernorm_after.weight: torch.Size([768])\nvit.encoder.layer.7.layernorm_after.bias: torch.Size([768])\nvit.encoder.layer.8.attention.attention.query.weight: torch.Size([768, 768])\nvit.encoder.layer.8.attention.attention.query.bias: torch.Size([768])\nvit.encoder.layer.8.attention.attention.key.weight: torch.Size([768, 768])\nvit.encoder.layer.8.attention.attention.key.bias: torch.Size([768])\nvit.encoder.layer.8.attention.attention.value.weight: torch.Size([768, 768])\nvit.encoder.layer.8.attention.attention.value.bias: torch.Size([768])\nvit.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])\nvit.encoder.layer.8.attention.output.dense.bias: torch.Size([768])\nvit.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])\nvit.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])\nvit.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])\nvit.encoder.layer.8.output.dense.bias: torch.Size([768])\nvit.encoder.layer.8.layernorm_before.weight: torch.Size([768])\nvit.encoder.layer.8.layernorm_before.bias: torch.Size([768])\nvit.encoder.layer.8.layernorm_after.weight: torch.Size([768])\nvit.encoder.layer.8.layernorm_after.bias: torch.Size([768])\nvit.encoder.layer.9.attention.attention.query.weight: torch.Size([768, 768])\nvit.encoder.layer.9.attention.attention.query.bias: torch.Size([768])\nvit.encoder.layer.9.attention.attention.key.weight: torch.Size([768, 768])\nvit.encoder.layer.9.attention.attention.key.bias: torch.Size([768])\nvit.encoder.layer.9.attention.attention.value.weight: torch.Size([768, 768])\nvit.encoder.layer.9.attention.attention.value.bias: torch.Size([768])\nvit.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])\nvit.encoder.layer.9.attention.output.dense.bias: torch.Size([768])\nvit.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])\nvit.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])\nvit.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])\nvit.encoder.layer.9.output.dense.bias: torch.Size([768])\nvit.encoder.layer.9.layernorm_before.weight: torch.Size([768])\nvit.encoder.layer.9.layernorm_before.bias: torch.Size([768])\nvit.encoder.layer.9.layernorm_after.weight: torch.Size([768])\nvit.encoder.layer.9.layernorm_after.bias: torch.Size([768])\nvit.encoder.layer.10.attention.attention.query.weight: torch.Size([768, 768])\nvit.encoder.layer.10.attention.attention.query.bias: torch.Size([768])\nvit.encoder.layer.10.attention.attention.key.weight: torch.Size([768, 768])\nvit.encoder.layer.10.attention.attention.key.bias: torch.Size([768])\nvit.encoder.layer.10.attention.attention.value.weight: torch.Size([768, 768])\nvit.encoder.layer.10.attention.attention.value.bias: torch.Size([768])\nvit.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])\nvit.encoder.layer.10.attention.output.dense.bias: torch.Size([768])\nvit.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])\nvit.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])\nvit.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])\nvit.encoder.layer.10.output.dense.bias: torch.Size([768])\nvit.encoder.layer.10.layernorm_before.weight: torch.Size([768])\nvit.encoder.layer.10.layernorm_before.bias: torch.Size([768])\nvit.encoder.layer.10.layernorm_after.weight: torch.Size([768])\nvit.encoder.layer.10.layernorm_after.bias: torch.Size([768])\nvit.encoder.layer.11.attention.attention.query.weight: torch.Size([768, 768])\nvit.encoder.layer.11.attention.attention.query.bias: torch.Size([768])\nvit.encoder.layer.11.attention.attention.key.weight: torch.Size([768, 768])\nvit.encoder.layer.11.attention.attention.key.bias: torch.Size([768])\nvit.encoder.layer.11.attention.attention.value.weight: torch.Size([768, 768])\nvit.encoder.layer.11.attention.attention.value.bias: torch.Size([768])\nvit.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])\nvit.encoder.layer.11.attention.output.dense.bias: torch.Size([768])\nvit.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])\nvit.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])\nvit.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])\nvit.encoder.layer.11.output.dense.bias: torch.Size([768])\nvit.encoder.layer.11.layernorm_before.weight: torch.Size([768])\nvit.encoder.layer.11.layernorm_before.bias: torch.Size([768])\nvit.encoder.layer.11.layernorm_after.weight: torch.Size([768])\nvit.encoder.layer.11.layernorm_after.bias: torch.Size([768])\nvit.layernorm.weight: torch.Size([768])\nvit.layernorm.bias: torch.Size([768])\nclassifier.weight: torch.Size([2, 768])\nclassifier.bias: torch.Size([2])\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}